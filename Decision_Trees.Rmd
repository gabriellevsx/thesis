---
title: "Decision Trees"
author: "Gabrielle Voiseux"
date: "17/05/2021"
output: html_document
---
# Decision Trees

## Set a goal

The data set is imbalanced, therefore, accuracy would not be a good metric, as a "null model"(always predicts no misstatement) would achieve a very high accuracy. 

Because our goal is to uncover observations that include misstattement, we should focus on sensitivity. Howver, because of the class imbalance, achieving high sensitivity will be tough. As such, let's aim for a sensitivity of 0.7. 

Overall, we want to achieve high sensitivity while still being able to detect observations that do not contain misstatement. Hence, a decent level of specificity, about 60%, is still needed. 
## Preparing the analysis

Let's start by loading the packages we will need.

```{r}
library(tidymodels)
library(dplyr)
library(themis)
library(splitstackshape)
```

We will also introduces functionality from the following packages in this notebook:

```{r}
library("rpart")
library("partykit")
library(rpart.plot)
```





## Model assessment setup 

```{r}

pval <- read.csv("pval_data.csv") %>% select(-X, -period, -fyear)

```

Before we analyse the data, we must plan ahead. We will need to tune our tree, which we will do using K-fold cross-validation. But let's also keep some data for testing our tuned tree. Therefore, we start by making a stratified train-test split. We keep 70% of the data for training and 30% for testing:

```{r}
pval$misstate <- as.factor(pval$misstate)
pval$sich <- as.factor(pval$sich)
pval$gvkey <- as.character(pval$gvkey)

pval <- pval[complete.cases(pval),]

nonbenf <- pval %>% select(-gvkey, -sich, -pval_1st, -pval_2nd, -pval_1st_2nd)
benf <- pval %>% select( -gvkey, -sich)

set.seed(466581)
nonbenf_split <-initial_split(nonbenf, prop = 7/10, strata = misstate)
nonbenf_split

# extract training and testing sets
nonbenf_train <- training(nonbenf_split)
nonbenf_test <- testing(nonbenf_split)


set.seed(466581)
nonbenf_folds <- vfold_cv(nonbenf_train, strata = misstate)


set.seed(466581)
benf_split <-initial_split(benf, prop = 7/10, strata = misstate)
benf_split

# extract training and testing sets
benf_train <- training(benf_split)
benf_test <- testing(benf_split)


set.seed(466581)
benf_folds <- vfold_cv(benf_train, strata = misstate)
```



# Classification trees using **tidymodels** - Let's consider how build a classification tree using **tidymodels**. 

Setting up a model - Here is what the model looks like, where we are going to tune the cost-complexity parameter:

```{r}
tree_nonbenf_model <- decision_tree(cost_complexity = tune()) %>% 
  set_mode("classification") %>% 
  set_engine("rpart")

tree_benf_model <- decision_tree(cost_complexity = tune()) %>% 
  set_mode("classification") %>% 
  set_engine("rpart")
```


Note that we will tune the cost-complexity parameter ($\alpha$ on the slides and `cp` in the **rpart** package). Also, we indicate that this is a classification problem.

Now we need to construct a pre-processing recipe. 
We do not have to create dummy variables or do normalization, but we will adjust for the class imbalance here. We use `step_downsample()` to select always 
We also have to take care to use only the variables we intend to use in the model:

```{r}
tree_nonbenf_recipe <- recipe(misstate ~ ., data = nonbenf_train) %>% 
  step_SMOTE(misstate)
tree_nonbenf_recipe



tree_benf_recipe <- recipe(misstate ~ ., data = benf_train) %>% 
  step_SMOTE(misstate)

```

We can combine the model and recipe into a workflow: 

```{r}
  
tree_wf <- workflow() %>% 
  add_recipe(tree_nonbenf_recipe) %>% 
  add_model(tree_nonbenf_model)


  
tree_benf_wf <- workflow() %>% 
  add_recipe(tree_benf_recipe) %>% 
  add_model(tree_benf_model)
```


## Performing cost-complexity pruning

We must first specify a tuning grid. 


```{r}
tree_grid <- tibble(cost_complexity = 9^seq(from = -7, to = 0, length.out = 20))
tree_grid
```


Now we can perform our 10-fold CV, calculating several metrics:


```{r}
class_metrics <- metric_set(mcc, accuracy, sensitivity, specificity)

tree_nonbenf_tune <- tree_wf %>% 
  tune_grid(resamples = nonbenf_folds,
            grid = tree_grid,
            metrics = class_metrics)

tree_benf_tune <- tree_benf_wf %>% 
  tune_grid(resamples = benf_folds,
            grid = tree_grid,
            metrics = class_metrics)

autoplot(tree_nonbenf_tune)
tree_nonbenf_tune_metrics <- tree_nonbenf_tune %>% collect_metrics()

autoplot(tree_benf_tune)
tree_benf_tune_metrics <- tree_benf_tune %>% collect_metrics()


```




```{r}
tree_nb_metrics_sub <- tree_nonbenf_tune_metrics %>% 
  pivot_wider(names_from = ".metric", 
              values_from = c("mean", "std_err")) %>% 
  filter(mean_mcc <0.55) %>% 
  arrange(desc(mean_mcc))
tree_nb_metrics_sub


tree_b_metrics_sub <- tree_benf_tune_metrics %>% 
  pivot_wider(names_from = ".metric", 
              values_from = c("mean", "std_err")) %>% 
  filter(mean_mcc <0.55) %>% 
  arrange(desc(mean_mcc))
tree_b_metrics_sub
```


Let's select `Model13' for both models.

```{r}
tree_selected <- tree_nb_metrics_sub %>% filter(.config == "Preprocessor1_Model13")
tree_selected

tree_bf_selected <- tree_b_metrics_sub %>% filter(.config == "Preprocessor1_Model13")
tree_bf_selected
```

The worflow can now be finalized.

```{r}
tree_wf_finalized <- tree_wf %>% finalize_workflow(tree_selected)

tree_wf_bf_finalized <- tree_benf_wf %>% finalize_workflow(tree_bf_selected)
```


The tuned workflows are again trained on the whole training data. 

```{r}
tree_wf_fit <- tree_wf_finalized %>% fit(nonbenf_train)
tree_wf_fit

tree_wf_bf_fit <- tree_wf_bf_finalized %>% fit(benf_train)
tree_wf_bf_fit

pull_workflow_fit(tree_wf_fit)$fit %>% rpart.plot(roundint = FALSE)


pull_workflow_fit(tree_wf_bf_fit)$fit %>% rpart.plot(roundint = FALSE)
```

  
The test set can be predicted. Metrics are calculated.

```{r}

class_metrics <- metric_set(mcc, accuracy, sensitivity, specificity)

tree_test_pred <- tree_wf_fit %>% predict(nonbenf_test)
tree_test_bf_pred <- tree_wf_bf_fit %>% predict(benf_test)

tree_test_metrics <- nonbenf_test %>% bind_cols(tree_test_pred) %>% 
  class_metrics(truth = misstate, estimate = .pred_class)

tree_test_bf_metrics <- benf_test %>% bind_cols(tree_test_bf_pred) %>% 
  class_metrics(truth = misstate, estimate = .pred_class)

tree_test_metrics
```

  
## Cross-validation - pruning 

```{r}
tree_formula <- misstate ~ .

set.seed(24356)
rpart_fit <- rpart(tree_formula, data = nonbenf_train, method = "class", 
                   control = rpart.control(cp = 0.003), parms = list(prior = c(0.5, 0.5)))

set.seed(24356)
rpart_bf_fit <- rpart(tree_formula, data = benf_train, method = "class", 
                   control = rpart.control(cp = 0.0003), parms = list(prior = c(0.5, 0.5)))
```

Results

```{r}
plotcp(rpart_fit)
printcp(rpart_fit)

plotcp(rpart_bf_fit)
printcp(rpart_bf_fit)
```


Plot the tree

```{r}
rpart_pruned <- rpart::prune(rpart_fit, cp = 0.0059563)
rpart.plot(rpart_pruned)
plot(as.party(rpart_pruned))


rpart_bf_pruned <- rpart::prune(rpart_bf_fit, cp = 0.00299463)
rpart.plot(rpart_bf_pruned)


```


# Results



```{r}
rpart_test_pred <- predict(rpart_pruned, newdata = nonbenf_test, type = "class")

rpart_bf_test_pred <- predict(rpart_bf_pruned, newdata = benf_test, type = "class")

rpart_test_metrics <- nonbenf_test %>% mutate(rpart_pred = rpart_test_pred) %>%
  class_metrics(truth = misstate, estimate = rpart_pred)

rpart_test_bf_metrics <- benf_test %>% mutate(rpart_bf_pred = rpart_bf_test_pred) %>%
  class_metrics(truth = misstate, estimate = rpart_bf_pred)

conf_bf <- benf_test %>% mutate(rpart_bf_pred = rpart_bf_test_pred) %>%
  conf_mat(truth = misstate, estimate = rpart_bf_pred)

conf <-  nonbenf_test %>% mutate(rpart_pred = rpart_test_pred) %>%
  conf_mat(truth = misstate, estimate = rpart_pred)



```





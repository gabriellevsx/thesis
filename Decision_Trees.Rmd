---
title: "Decision Trees"
author: "Gabrielle Voiseux"
date: "17/05/2021"
output: html_document
---
# Decision Trees

## Set a goal

The data set is imbalanced, therefore, accuracy would not be a good metric, as a "null model"(always predicts no misstatement) would achieve a very high accuracy. 

Because our goal is to uncover observations that include misstattement, we should focus on sensitivity. Howver, because of the class imbalance, achieving high sensitivity will be tough. As such, let's aim for a sensitivity of 0.7. 

Overall, we want to achieve high sensitivity while still being able to detect observations that do not contain misstatement. Hence, a decent level of specificity, about 60%, is still needed. 
## Preparing the analysis

Let's start by loading the packages we will need.

```{r}
 

library(tidymodels)
library(dplyr)
library(themis)
library(splitstackshape)
```

We will also introduces functionality from the following packages in this notebook:

```{r}
library("rpart")
library("partykit")
library(rpart.plot)
```





## Model assessment setup 

```{r}
fraud <- read.csv("final_data.csv") %>% select(-X)
```

Before we analyse the data, we must plan ahead. We will need to tune our tree, which we will do using K-fold cross-validation. But let's also keep some data for testing our tuned tree. Therefore, we start by making a stratified train-test split. We keep 70% of the data for training and 30% for testing:

```{r}
fraud$misstate <- as.factor(fraud$misstate)
fraud$bf_1 <- as.factor(fraud$bf_1)
fraud$bf_2 <- as.factor(fraud$bf_2)
fraud$bf_3 <- as.factor(fraud$bf_3)
fraud$bf_agr <- as.factor(fraud$bf_agr)
fraud$fyear <- as.factor(as.character(fraud$fyear))
fraud$period <- as.factor(fraud$period)
fraud$sich <- as.factor(fraud$sich)
fraud$gvkey <- as.character(fraud$gvkey)


fraud <- fraud[complete.cases(fraud),]
nonbenf <- fraud %>% select(-gvkey, -sich, -bf_1, -bf_2, -bf_3, -bf_agr, -fyear, -period)
benf <- fraud %>% select(-gvkey, -sich, -fyear, -period)

set.seed(466581)
nonbenf_split <-initial_split(nonbenf, prop = 7/10, strata = misstate)
nonbenf_split

# extract training and testing sets
nonbenf_train <- training(nonbenf_split)
nonbenf_test <- testing(nonbenf_split)


set.seed(466581)
nonbenf_folds <- vfold_cv(nonbenf_train, strata = misstate)




set.seed(466581)
benf_split <-initial_split(benf, prop = 7/10, strata = misstate)
benf_split

# extract training and testing sets
benf_train <- training(benf_split)
benf_test <- testing(benf_split)


set.seed(466581)
benf_folds <- vfold_cv(benf_train, strata = misstate)
```



# Classification trees using **tidymodels** - Let's consider how build a classification tree using **tidymodels**. 

Setting up a model - Here is what the model looks like, where we are going to tune the cost-complexity parameter:

```{r}
tree_nonbenf_model <- decision_tree(cost_complexity = tune()) %>% 
  set_mode("classification") %>% 
  set_engine("rpart")

tree_benf_model <- decision_tree(cost_complexity = tune()) %>% 
  set_mode("classification") %>% 
  set_engine("rpart")
```


Note that we will tune the cost-complexity parameter ($\alpha$ on the slides and `cp` in the **rpart** package). Also, we indicate that this is a classification problem.

Now we need to construct a pre-processing recipe. 
We do not have to create dummy variables or do normalization, but we will adjust for the class imbalance here. We use `step_downsample()` to select always 
We also have to take care to use only the variables we intend to use in the model:

```{r}
tree_nonbenf_recipe <- recipe(misstate ~ ., data = nonbenf_train) %>% 
  step_downsample(misstate)
tree_nonbenf_recipe



tree_benf_recipe <- recipe(misstate ~ ., data = benf_train) %>% 
  step_dummy(bf_1, bf_2, bf_3, bf_agr) %>%
  step_downsample(misstate)

sum(is.na(nonbenf_train))
```

We can combine the model and recipe into a workflow: 

```{r}
  
tree_wf <- workflow() %>% 
  add_recipe(tree_nonbenf_recipe) %>% 
  add_model(tree_nonbenf_model)


  
tree_benf_wf <- workflow() %>% 
  add_recipe(tree_benf_recipe) %>% 
  add_model(tree_benf_model)
```


## Performing cost-complexity pruning

First, we will need tuning grid. Let's use the following values in our grid search (can be changed later on)


```{r}
tree_grid <- tibble(cost_complexity = 9^seq(from = -7, to = 0, length.out = 20))
tree_grid
```


Now we can perform our 10-fold CV, calculating several metrics:


```{r}
class_metrics <- metric_set(accuracy, kap, sensitivity, specificity)

tree_nonbenf_tune <- tree_wf %>% 
  tune_grid(resamples = nonbenf_folds,
            grid = tree_grid,
            metrics = class_metrics)

tree_benf_tune <- tree_benf_wf %>% 
  tune_grid(resamples = benf_folds,
            grid = tree_grid,
            metrics = class_metrics)

autoplot(tree_nonbenf_tune)
tree_nonbenf_tune_metrics <- tree_nonbenf_tune %>% collect_metrics()
write.csv(tree_nonbenf_tune_metrics, file="tree_nonbenf_tune_metrics.csv")

autoplot(tree_benf_tune)
tree_benf_tune_metrics <- tree_benf_tune %>% collect_metrics()
write.csv(tree_benf_tune_metrics, file="tree_benf_tune_metrics.csv")


```





The values of $\alpha$ which give a mean estimated sensitivity between 0.65 and 0.75 are:

```{r}
tree_metrics_sub <- tree_benf_tune_metrics %>% 
  pivot_wider(names_from = ".metric", 
              values_from = c("mean", "std_err")) %>% 
  filter(mean_spec > 0.55, mean_spec < 0.76) %>% 
  arrange(desc(mean_spec))
tree_metrics_sub
```


Let's select `Model10', which achieves about 71% sensitivity and 67% specificity:

```{r}
tree_selected <- tree_metrics_sub %>% filter(.config == "Preprocessor1_Model14")
tree_selected

tree_bf_selected <- tree_metrics_sub %>% filter(.config == "Preprocessor1_Model14")
tree_bf_selected
```

Now we can finalize our workflow using our selected value of the tuning parameter as:

```{r}
tree_wf_finalized <- tree_wf %>% finalize_workflow(tree_selected)

tree_wf_bf_finalized <- tree_benf_wf %>% finalize_workflow(tree_bf_selected)
```


The tuned workflow can be trained on all the training data with -Note that only 1358 observations were used to train the tree, since downsampling was used.

```{r}
tree_wf_fit <- tree_wf_finalized %>% fit(nonbenf_train)
tree_wf_fit

tree_wf_bf_fit <- tree_wf_bf_finalized %>% fit(benf_train)
tree_wf_bf_fit

pull_workflow_fit(tree_wf_fit)$fit %>% rpart.plot(roundint = FALSE)


pull_workflow_fit(tree_wf_bf_fit)$fit %>% rpart.plot(roundint = FALSE)
```

  

To be able to compare our results here to other methods in subsequent notebooks, we predict the test set and calculate some metrics for those predictions. We will use the following metrics:
 
Here are the predicted classess:

```{r}

class_metrics <- metric_set(accuracy, kap, bal_accuracy, sensitivity, specificity)

tree_test_pred <- tree_wf_fit %>% predict(nonbenf_test)
tree_test_bf_pred <- tree_wf_bf_fit %>% predict(benf_test)

tree_test_metrics <- nonbenf_test %>% bind_cols(tree_test_pred) %>% 
  class_metrics(truth = misstate, estimate = .pred_class)

tree_test_bf_metrics <- benf_test %>% bind_cols(tree_test_bf_pred) %>% 
  class_metrics(truth = misstate, estimate = .pred_class)

tree_test_metrics
```

  


## Cross-validation for pruning 

We will specify the model using a complete formula. Here is a formula which uses all variables 

```{r}
tree_formula <- misstate ~ .

set.seed(24356)
rpart_fit <- rpart(tree_formula, data = nonbenf_train, method = "class", 
                   control = rpart.control(cp = 0.003), parms = list(prior = c(0.5, 0.5)))

set.seed(24356)
rpart_bf_fit <- rpart(tree_formula, data = benf_train, method = "class", 
                   control = rpart.control(cp = 0.003), parms = list(prior = c(0.5, 0.5)))
```

The results for the CV are as follows: 

```{r}
plotcp(rpart_fit)
printcp(rpart_fit)

plotcp(rpart_bf_fit)
printcp(rpart_bf_fit)
```


We can now prune the tree and plot it. Let's use the tree with 15 splits:

```{r}
rpart_pruned <- rpart::prune(rpart_fit, cp = 0.0063)
rpart.plot(rpart_pruned)
plot(as.party(rpart_pruned))


rpart_bf_pruned <- rpart::prune(rpart_bf_fit, cp = 0.006)
rpart.plot(rpart_bf_pruned)


```


# Results

Here are the predicted classes for the test data:

```{r}
rpart_test_pred <- predict(rpart_pruned, newdata = nonbenf_test, type = "class")

rpart_bf_test_pred <- predict(rpart_bf_pruned, newdata = benf_test, type = "class")

rpart_test_metrics <- nonbenf_test %>% mutate(rpart_pred = rpart_test_pred) %>%
  class_metrics(truth = misstate, estimate = rpart_pred)

rpart_test_bf_metrics <- benf_test %>% mutate(rpart_bf_pred = rpart_bf_test_pred) %>%
  class_metrics(truth = misstate, estimate = rpart_bf_pred)


write.csv(rpart_test_metrics, "dt_final_nb.csv")

write.csv(rpart_test_bf_metrics, "dt_final_b.csv")

```

